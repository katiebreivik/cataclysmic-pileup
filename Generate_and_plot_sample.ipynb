{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9d2d1-6432-4ecc-b7c7-e5bded7f220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "from legwork import utils, strain, psd, source\n",
    "from astropy.coordinates import SkyCoord\n",
    "#from CV_pop import sample_porb_from_Knigge_2011\n",
    "import scipy.stats as stats\n",
    "from astropy.coordinates import get_body_barycentric\n",
    "from astropy.time import Time\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.ndimage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff06e17-5aa9-4fd0-bd31-e4abb1bf1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_distance = 1000 # parsecs: must be larger than 150!\n",
    "!python3 CV_pop.py --max-distance {max_distance} --mu-m1 0.7 --sigma-m1 0.001 --sigma-m2 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e316f-8977-4e38-a0fc-68bda795b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"dat_maxDistance_{max_distance}.txt\"\n",
    "dat = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CDF of the orbital period distributiondistribution   \n",
    "\n",
    "#lpdist = pd.read_csv(\"lpdist3.out\", delim_whitespace=True, header=None, names=['logp', 'CDF'])\n",
    "\n",
    "\n",
    "# Smooth the CDF with a kde to make it more continuous\n",
    "#pdist['CDF'] = scipy.ndimage.gaussian_filter1d(lpdist['CDF'], 5)\n",
    "\n",
    "\n",
    "#lpdist['porb'] = 10**lpdist['logp'] # convert logp to porb\n",
    "#lpdist['porb'] = lpdist['porb'] / 60 # convert minutes to hours\n",
    "\n",
    "#plt.scatter(lpdist.porb, lpdist.CDF, label='CDF')\n",
    "\n",
    "# sample from the CDF\n",
    "#nCV = 100000\n",
    "#pp = np.random.uniform(0, 1, nCV)\n",
    "#porb = np.interp(pp, lpdist.CDF, lpdist.porb)\n",
    "\n",
    "#plt.hist(porb, bins=200, density=True, histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0d62a-2039-4691-8a69-3cde69de7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.rename(columns={'# m1[Msun]':'m1',\n",
    "                          ' m2[Msun]':'m2',\n",
    "                          ' inclination[rad]':'inc',\n",
    "                          ' f_gw[Hz]':'f_gw',\n",
    "                          ' x_gal[kpc]':'x', \n",
    "                          ' y_gal[kpc]':'y', \n",
    "                          ' z_gal[kpc]':'z',\n",
    "                          ' Pala_reassigned' : 'pala'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff7453-d280-47d9-aff6-ee597852b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KT = pd.read_csv('kniggeTable.csv')\n",
    "PT = pd.read_hdf('Pala_2020_dat_combo.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3659f-518c-4464-8178-70898ea2789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(1/(dat.f_gw/2) / 3600, bins=200, density=True, histtype='step', label=f'{max_distance} pc sample')\n",
    "#plt.hist(KT.Per, density=True, bins=50, histtype='step', label='Knigge+2011 histogram')\n",
    "plt.hist(PT.porb/60, density=True, histtype='step', bins=50, label='Pala+2020 histogram')\n",
    "plt.xlabel('porb [hr]', size=12)\n",
    "plt.ylabel('count density', size=12)\n",
    "plt.legend(prop={'size':12})\n",
    "plt.tick_params('both', labelsize=10)\n",
    "plt.xlim(1, 5.5)\n",
    "plt.savefig(\"porb.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ab0ca-be1e-49ea-98a1-8577a3b99037",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.scatter(dat['f_gw']*1000, dat['m2'], s=5)\n",
    "plt.scatter(dat.loc[dat.pala > 0].f_gw*1000, dat.loc[dat.pala > 0].m2, s=15)\n",
    "plt.xlabel('GW frequency [mHz]', size=12)\n",
    "plt.ylabel('Donor mass [$M_{\\odot}$]', size=12)\n",
    "plt.tick_params('both', labelsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3abd40-12bb-43c6-8d0c-36c26845226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = SkyCoord(dat.x, dat.y, dat.z, unit=u.kpc, frame='galactocentric')\n",
    "\n",
    "dist = c.icrs.distance\n",
    "\n",
    "mc = utils.chirp_mass(dat['m1'].values * u.Msun, dat['m2'].values * u.Msun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5eb616-011c-4248-b366-6123e2b7739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = source.Source(m_1=dat['m1'].values * u.Msun, m_2=dat['m2'].values * u.Msun, \n",
    "                        ecc=np.zeros_like(dat['m1'].values), dist=dist, f_orb=dat['f_gw'].values/2 * u.Hz,\n",
    "                        interpolate_g=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ec514-127f-4324-9385-ff7ffd26be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = sources.get_snr()\n",
    "ASD = strain.h_0_n(m_c=mc, f_orb=dat['f_gw'].values/2 * u.Hz,  \n",
    "                   ecc=np.zeros(len(mc)), dist=dist, \n",
    "                   n=2, position=None, polarisation=None, \n",
    "                   inclination=None, interpolated_g=None) * np.sqrt(4 * 3.155e7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544098ab-c60d-4b8c-9d57-bb6701ebc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind, = np.where(dat.pala == 1)\n",
    "indD, = np.where(dat.pala==2)\n",
    "\n",
    "c = SkyCoord(dat.x, dat.y, dat.z, unit=u.kpc, frame='galactocentric', representation_type='cartesian')\n",
    "d = c.transform_to('barycentrictrueecliptic')\n",
    "d_rounded = np.around(d.distance, decimals=5)\n",
    "\n",
    "indCheck150, = np.where(d_rounded <= 0.15*u.kpc)\n",
    "\n",
    "n150 = len(indCheck150)\n",
    "V150 = 4/3 * np.pi * 150**3\n",
    "print(f\"The space density within 150 pc is: {n150/V150}\")\n",
    "\n",
    "# Sanity check: what is the space density of the sample? Should be just under 4.8e-6 N/pc^3\n",
    "Nsources = d.shape[0] # total number of sources in the sample\n",
    "Vol = np.pi*max_distance**3 # total volume of the sample in pc^3\n",
    "print(\"Total number of sources:\",Nsources)\n",
    "print(\"Total volume:\", Vol)\n",
    "print('Recovered space density:', Nsources/Vol)\n",
    "\n",
    "# Another check is to plot the distribution of sources in the z direction\n",
    "\n",
    "print(d.distance.min())\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist((np.array(d.distance)), bins=50, density=True, histtype='step', label='800 pc sample')\n",
    "plt.xlabel('distance [kpc]', size=12)\n",
    "\n",
    "frequency_range = frequency_range=np.logspace(-5, 0, 1000) * u.Hz\n",
    "LISA = psd.lisa_psd(frequency_range, approximate_R=False, confusion_noise='robson19')\n",
    "\n",
    "\n",
    "# Print orbital periods of S/N>4 sources.\n",
    "palaSNR = snr[ind]\n",
    "indH = np.where(palaSNR > 4)\n",
    "#print(palaSNR)\n",
    "print(palaSNR[indH])    \n",
    "print(1/(dat.f_gw[ind[indH]]/2) / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2a558-14b2-4fd4-8e15-77254a5c3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "plt.plot(frequency_range, LISA**0.5, lw=1, c='black')   \n",
    "plt.scatter(dat.f_gw, ASD, s=10, label=f'{max_distance} pc sample',alpha=1/10)\n",
    "#plt.scatter(dat.f_gw[indD], ASD[indD], s=10, label='150 pc sample')\n",
    "plt.scatter(dat.loc[dat.pala == 2].f_gw, ASD[indD], s=20, label='150pc')\n",
    "plt.scatter(dat.loc[dat.pala == 1].f_gw, ASD[ind], s=20, label='Pala+2020')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(4e-5, 2e-3)\n",
    "plt.ylim(1e-19, 1e-16)\n",
    "plt.xlabel('GW frequency [Hz]', size=12)\n",
    "plt.ylabel('ASD [Hz$^{-1/2}$]', size=12)\n",
    "plt.tick_params('both', labelsize=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712692b-6497-4624-b7b5-1f615a3bf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat.loc[dat.pala == 2]), len(dat.loc[dat.pala==1]), len(dat.loc[dat.pala == 2]) + len(dat.loc[dat.pala==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd7f39-43ad-4e46-9364-7ffb1b3ba052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7796-1859-45bd-9504-ac4cce5798bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "\n",
    "# Compute the 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(np.squeeze(dat.f_gw), np.squeeze(ASD), bins=50)\n",
    "\n",
    "print(hist.max())\n",
    "\n",
    "# Define the contour levels\n",
    "#levels = np.linspace(100, hist.max(), 10)\n",
    "levels = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "          \n",
    "# Smooth the data using a Gaussian filter\n",
    "sigma = 2  # Adjust this parameter to control the smoothing strength\n",
    "smoothed_hist = scipy.ndimage.gaussian_filter(hist, sigma)\n",
    "\n",
    "\n",
    "# Plot the contour lines\n",
    "plt.contour(x_edges[:-1], y_edges[:-1], smoothed_hist.T, levels=levels)\n",
    "\n",
    "\n",
    "# Plot the rest\n",
    "plt.plot(frequency_range, LISA**0.5, lw=1, c='black')   \n",
    "plt.scatter(dat.f_gw, ASD, s=5, alpha= 0.5, label=f'{max_distance} pc sample')\n",
    "#plt.scatter(dat.f_gw[indD], ASD[indD], s=10, label='150 pc sample')\n",
    "plt.scatter(dat.loc[dat.pala == 2].f_gw, ASD[indD], s=20, label='150pc')\n",
    "plt.scatter(dat.loc[dat.pala == 1].f_gw, ASD[ind], s=20, label='Pala+2020')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(9e-5, 1e-3)\n",
    "plt.ylim(1e-18, 3e-17)\n",
    "plt.xlabel('GW frequency [Hz]', size=12)\n",
    "plt.ylabel('ASD [Hz$^{-1/2}$]', size=12)\n",
    "plt.tick_params('both', labelsize=10)\n",
    "\n",
    "\n",
    "plt.savefig(\"asd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f7def",
   "metadata": {},
   "source": [
    "# LISA Processing\n",
    "\n",
    "The LISA processing steps require two different installations of the `ldasoft` repository. \n",
    "The current release has the spectral modeling algorithm `noise_mcmc` used for fitting the confusion noise level of the simulated data. To produce the simulated data you must roll back to `ldasoft` git commit `5e80706a7` for the `FisherGalaxy` install. For the user man\n",
    "\n",
    "The input WDWD population used here comes from the LISA Data Challenge (LDC) _Sangria_ training data set [LDC2a-v1](https://lisa-ldc.lal.in2p3.fr/challenge2a)\n",
    "\n",
    "Generating the simulated LISA data and fitting for the resultant noise spectra takes O(hours). For convenience we include the output data files to produce the confusion noise spectrum plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input LDC galaxy containing the CV population\n",
    "\n",
    "#!cat full_galaxy.dat \"dat_maxDistance_{max_distance}_GW.txt\" > full_galaxy_plus_CVs.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13089bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LISA data using LDC-only galaxy, estimate which sources are resolvable, \n",
    "# create the residual galaxy after removal of resolvable sources, and reformat output \n",
    "# for the confusion noise plot\n",
    "\n",
    "#!bash run.sh\n",
    "#!awk '{print $1,$4,$5,$6,$7}' Galaxy_XAE.dat > lisa_data_no_cvs.dat\n",
    "#!awk '{print $1,$4,$5,$6,$7}' Galaxy_XAE_R1.dat > lisa_residual_no_cvs.dat\n",
    "#!cp Confusion_XAE_1.dat noise.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian spectral fittingn code on residual to get noise level, and reformat output for plot\n",
    "\n",
    "#!noise_mcmc --data lisa_residual_no_cvs.dat --duration 125829120 --fmin 0.0001 --samples 524288 --sangria --steps 1000 \n",
    "#!mv data/current_interpolated_spline_points.dat psd_no_cvs.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LISA data using LDC-only galaxy with CVs include, estimate which sources are resolvable, \n",
    "# create the residual galaxy after removal of resolvable sources, and reformat output \n",
    "# for the confusion noise plot\n",
    "\n",
    "#!bash run_cvs.sh\n",
    "#!awk '{print $1,$4,$5,$6,$7}' Galaxy_XAE_R1.dat > lisa_residual_w_cvs.dat\n",
    "#!awk '{print $1,$4,$5,$6,$7}' Galaxy_XAE.dat > lisa_data_w_cvs.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9854a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian spectral fittingn code on residual to get noise level, and reformat output for plot\n",
    "\n",
    "#!noise_mcmc --data lisa_residual_w_cvs.dat --duration 125829120 --fmin 0.0001 --samples 524288 --sangria --steps 1000 \n",
    "#!mv data/current_interpolated_spline_points.dat psd_w_cvs.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The money plot\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "                   \n",
    "data=np.loadtxt('lisa_data_w_cvs.dat')\n",
    "residual=np.loadtxt('lisa_residual_w_cvs.dat')\n",
    "cv_confusion=np.loadtxt('psd_w_cvs.dat')\n",
    "wdwd_confusion=np.loadtxt('psd_no_cvs.dat')\n",
    "noise=np.loadtxt('noise.dat')\n",
    "\n",
    "plt.plot(data[:,0],np.sqrt(data[:,1]*data[:,1]+data[:,2]*data[:,2]),label='data',color=\"gray\")   \n",
    "plt.plot(residual[:,0],np.sqrt(residual[:,1]*residual[:,1]+residual[:,2]*residual[:,2]),label='residual',color=\"lightgray\")   \n",
    "plt.plot(cv_confusion[:,0],np.sqrt(cv_confusion[:,1]),label='cv confusion',color=\"tab:blue\", linewidth=3)   \n",
    "plt.plot(wdwd_confusion[:,0],np.sqrt(wdwd_confusion[:,1]),label='wdwd confusion',color=\"tab:orange\", linewidth=3)   \n",
    "plt.plot(noise[:,0],np.sqrt((noise[:,3]-noise[:,4])/2),label='instrument noise',color=\"black\", linestyle='dashed')   \n",
    "\n",
    "plt.legend(prop={'size':12})\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(1e-4, 4e-3)\n",
    "plt.ylim(1e-24, 1e-18)\n",
    "\n",
    "plt.xlabel('GW frequency [Hz]', size=12)\n",
    "plt.ylabel('ASD [Hz$^{-1/2}$]', size=12)\n",
    "plt.tick_params('both', labelsize=10)\n",
    "\n",
    "\n",
    "plt.savefig(\"spectra.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238b264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2e587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
